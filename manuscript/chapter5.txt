-# Reading Data Advanced

# Understanding index

## Index

Soon enough you will realize that `filter` is slow. If you have a table
with more than 100,000 records, `filter` stops working. All of that is because
we haven't used index yet. Without an index, we cannot even order data.

    r.db("foodb").table("compounds_foods").orderBy(r.desc("id"))
    #->
    RqlRuntimeError: Array over size limit `100000` in:
    r.db("foodb").table("compounds_foods").orderBy(r.desc("id"))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Without an index, RethinkDB holds all data in memory and sorts or filters in
memory. A limit has to lie somewhere else. 100,000 is the magic number of the limit
RethinkDB set to read data without index.


In order to properly fetching data, we have to create an Index. And in real application, we almost always ending up creating index on MySQL  to fetch data efficiently. 

We have two kinds of indexes in RethinkDB

* primary index: our ID key is this. This index is created automatically by
RethinkDB. Coming back to the above query, if we change it to use primary `index`

    r.db("foodb").table("compounds_foods").orderBy({index: r.desc("id")})

`id` field is always indexed automatically.

* secondary index:

Seconday index is the index we created ourselve on one or many fields. Secondary
index can be simple, just index the value of fields directly, or doing some
pre-calculate on data before indexing.

While index helps to decrease the reading time, it decreases writing time, and also cost
storage space. It reduces write performance because whenever we insert a
document, the index has to be calculated and written into the database.

RethinkDB supports those kinds of index:

*Simple
:indexes based on the value of a single field.
*Compound
:indexes based on multiple fields.
*Multi 
:indexes based on arrays of values.
*Indexes 
:based on arbitrary expressions.

So now you know what index is. But the sad news is that filter cannot use those secondary index. For that purpose, we have to use other functions: `getAll` and `between`. 
### Creating index

Let's start with simple index first.

## Simple index

As its name, simple index is simply a single field. Let's say, we want to find all `compounds_foods` whose name contains `banana`. We cannot use `filter` here because this table has more than 100,000 item. `filter` also doesn't use index. Let's meet `getAll`. `getAll` grabs all documents where a given key match the index which we specified.

First, we create index follow this syntax

    table.index_create(index_name[, index_function][, :multi => false]) → object

Apply it on our case, for a single column:

    r.db("foodb")
      .table("compounds_foods")
      .indexCreate("match_orig_food_common_name", r.row.match)

If we don't pass an index function, RethinkDB will try to create index for the column that has the same name as requested index name.

Time to use it:

    r.db("foodb")
      .table("compounds_foods")
      .getAll('Bananas', {index:'orig_food_common_name'})
    #=> Executed in 10ms. No results returned.

No result. It's strange that we have no document where its
orig_food_common_name doesn't contain banana. Why so? Well, the simple
index does an exact match, or in other world, it's an equal comparison.
Let's try an exact match:

    r.db("foodb")
      .table("compounds_foods")
      .getAll('Bananas, raw', {index:'orig_food_common_name'})
    #=> Executed in 69ms. 40 rows returned, 40 displayed, more available
    {
    "citation":  "USDA" ,
    "citation_type":  "DATABASE" ,
    "compound_id": 2100 ,
    "created_at": Tue Jan 03 2012 18:33:15 GMT-08:00 ,
    "creator_id": null ,
    "food_id": 208 ,
    "id": 257686 ,
    "orig_citation": null ,
    "orig_compound_id":  "262" ,
    "orig_compound_name":  "Caffeine" ,
    "orig_content":  "0.0" ,
    "orig_food_common_name":  "Bananas, raw" ,
    "orig_food_id":  "09040" ,
    "orig_food_part": null ,
    "orig_food_scientific_name": null ,
    "orig_max": null ,
    "orig_method": null ,
    "orig_min": null ,
    "orig_unit":  "mg" ,
    "orig_unit_expression": null ,
    "updated_at": Tue Jan 03 2012 18:33:15 GMT-08:00 ,
    "updater_id": null
    }

We can pass multiple keys to `getAll` to have an `or` effect. Meaning RethinkDB
returns document where the index match any of value that we pass.

    r.db("foodb")
      .table("compounds_foods")
      .getAll('Bananas, raw', 'Yoghurt with pear and banana', 'Alfalfa seeds',{index:'orig_food_common_name'})

We can chain to `count` to count how many document we have

    r.db("foodb")
      .table("compounds_foods")
      .getAll('Bananas, raw', 'Yoghurt with pear and banana', 'Alfalfa seeds',{index:'orig_food_common_name'})
      .count()
    #=> 256

Not only used to find document, index can also be used for sorting as well. To sort,
we call `orderBy` and passing index name.

    r.db("foodb").table("compounds_foods")
      .orderBy({index: "orig_food_common_name"})

When passing index, we can wrap it in other expression to change the ordering:

    r.db("foodb").table("compounds_foods")
      .orderBy({index: r.desc("orig_food_common_name")})
      .withFields("orig_food_common_name")
    #=> 
    {
    "orig_food_common_name":  "Zwieback"
    } {
    "orig_food_common_name":  "Zwieback"
    } {
    "orig_food_common_name":  "Zwieback"
    }

Using `withFields`, we can pass a list of field to choose what we want to get
back.

Because index can be used to sort, we can use it to find the value between a
range. In RethinkDB, `between` syntax is:

    table.between(lowerKey, upperKey[, {index: 'id', leftBound: 'closed',
rightBound: 'open'}]) → selection

As you can see, we can only use between on table type. So, take note that
using this, we can find the document between "Apple" and "Banana" range.

    r.db("foodb").table("compounds_foods")
      .between("Apple", "Banana", {index: 'orig_food_common_name'})
      .orderBy({index: r.desc("orig_food_common_name")})
      .withFields("orig_food_common_name")

Without specifying an index, `between` operates on primary index, many RethinkDB 
function has the same behaviour

    r.db("foodb").table("compounds_foods")
      .between(1, 200)
      .count()
    #=> 198

This works when we want to find data based on a single field. How about find
value base on multiple field? Let's meet compound index

## Compound index

Compound index is created by using value of multiple fields. It's very similar to
single index in syntax, just different on how many fields we pass to index
create. Let's take a look at `compounds_foods` table, it contains relationship of
`foods` and `compounds`. We will learn more about JOIN later. For now, let's say
we want to find all `compounds_foods` document where its `compound_id` is 3524
and `food_id` is 287. We are finding on two columns, so we need an index
contains those 2 columns:

    r.db("foodb").table("compounds_foods")
      .indexCreate("compound_food_id", [r.row(row"compound_id"), r.row("food_id")]) 

The only different with simple index is that we have to pass an array of field to
create index. Let's try it:

    r.db("foodb").table("compounds_foods")
      .getAll([354,287], {index: 'compound_food_id'})
    #=>
    RqlRuntimeError: Index `compound_food_id` on table `foodb.compounds_foods` was
    accessed before its construction was finished in:
    r.db("foodb").table("compounds_foods").getAll([354, 287], {index:
    "compound_food_id"})
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We got an error. Looks like the index is not completed created yet. The table
has 

We can query index status:

    r.db("foodb").table("compounds_foods")
      .indexStatus('compound_food_id')
    #=>
    [
    {
    "blocks_processed": 13192 ,
    "blocks_total": 15437 ,
    "function": <binary, 408 bytes, "24 72 65 71 6c 5f..."> ,
    "geo": false ,
    "index":  "compound_food_id" ,
    "multi": false ,
    "outdated": false ,
    "ready": false
    }
    ]

The field ready is false. We can only wait until it finishes. This table is very
big. We can verify:

    r.db("foodb").table("compounds_foods").count()
    #=> 737089

Let's just wait for a bit, make a cup of coffee and come back :). When it's ready,
you should see:

    r.db("foodb").table("compounds_foods")
      .indexStatus('compound_food_id')

    [
    {
    "function": <binary, 408 bytes, "24 72 65 71 6c 5f..."> ,
    "geo": false ,
    "index":  "compound_food_id" ,
    "multi": false ,
    "outdated": false ,
    "ready": true
    }
    ]

Now, try it:
    
    r.db("foodb").table("compounds_foods")
      .getAll([21477,899], {index: 'compound_food_id'})
    #=> Executed in 7ms. 1 row returned
    {
    "citation":  "DFC CODES" ,
    "citation_type":  "DATABASE" ,
    "compound_id": 21477 ,
    "created_at": Tue Sep 11 2012 16:12:30 GMT-07:00 ,
    "creator_id": null ,
    "food_id": 899 ,
    "id": 740574 ,
    "orig_citation": null ,
    "orig_compound_id": null ,
    "orig_compound_name": null ,
    "orig_content": null ,
    "orig_food_common_name":  "Meats" ,
    "orig_food_id":  "WI8000" ,
    "orig_food_part": null ,
    "orig_food_scientific_name": null ,
    "orig_max": null ,
    "orig_method": null ,
    "orig_min": null ,
    "orig_unit": null ,
    "orig_unit_expression": null ,
    "updated_at": Tue Sep 11 2012 16:12:30 GMT-07:00 ,
    "updater_id": null
    }

With above indexing approach, you found that you have to create a dedicated index
for whatever you want to find. An index contains a single value for a document:
either a single value of field, or an order set of value in case of compound
index. However, life is not that simple. Let's looking at `users` table. It contains
a list of users and their three most favourite foods, stored in field
`favfoods`. That's a single column but it holds many elements. Because of that,
we cannot simply answer the question of who liked `Mushroom`:

let's create an index

    r.db("foodb").table("users").indexCreate('favfoods')

Try to find all users who liked mushrooms.

    r.db("foodb").table("users")
      .getAll('Mushrooms', {index: 'favfoods'})
    #=> Executed in 6ms. No results returned.

Why so? Because since we index the whole field as a single value, we have to match the
whole value of field:

    r.db("foodb").table("users")
      .getAll(["Edible shell" ,
        "Clupeinae (Herring, Sardine, Sprat)" ,
        "Deer" ,
        "Perciformes (Perch-like fishes)" ,
        "Bivalvia (Clam, Mussel, Oyster)"], {index: 'favfoods'})
    #=>
    {
    "favfoods": [
    "Edible shell" ,
    "Clupeinae (Herring, Sardine, Sprat)" ,
    "Deer" ,
    "Perciformes (Perch-like fishes)" ,
    "Bivalvia (Clam, Mussel, Oyster)"
    ] ,
    "id":  "1dd8059c-82ca-4345-9d75-eaa0f8edbf48" ,
    "name":  "Arthur Hegmann"
    }

*Multi index* is used to solve the above question: who likes Banana. A multi index
is used on multiple value, or in other word, an array of value. When RethinkDB
see that we want to use a *multi index*, it tries to loop over all values in the
array of index, and match to each of element of the array.

To create a multi index, all we have to do is to pass the option flag:` multi:
true`


    r.db("foodb").table("users").indexCreate('favfoods_multi', r.row("favfoods"), {multi: true})

Now, let's try it:

    r.db("foodb").table("users")
      .getAll('Mushrooms', {index: 'favfoods_multi'})
    #=>
    {
    "favfoods": [
    "Milk substitute" ,
    "Mushrooms" ,
    "Nuts" ,
    "Hummus" ,
    "Soft-necked garlic"
    ] ,
    "id":  "47110a8f-3c2c-46b8-96d8-244747c1818b" ,
    "name":  "Annabelle Lindgren"
    }

If you notice, we have to pass `r.row(favfoods)` to create an index. Remember
that in order to create an index where its name doesn't match any field, we have to pass
an expression or an anonymous fuction to `indexCreate` to caculate its value.
But we defined `favfoods` index before, so now we cannot create other index with the
same name. We can go back, delete index to clear thing up and save our
namespace:

    r.db("foodb").table("users").indexDrop('favfoods')
    #=> 
    { "dropped": 1 } 
    r.db("foodb").table("users").indexDrop('favfoods_multi')
    #=> 
    { "dropped": 1 } 

Now, let's create a multile index with the same field name:

    r.db("foodb").table("users").indexCreate('favfoods', {multi: true})
    r.db("foodb").table("users").getAll('Mushrooms', {index: 'favfoods'})
    #=>
    {
    "favfoods": [
    "Milk substitute" ,
    "Mushrooms" ,
    "Nuts" ,
    "Hummus" ,
    "Soft-necked garlic"
    ] ,
    "id":  "47110a8f-3c2c-46b8-96d8-244747c1818b" ,
    "name":  "Annabelle Lindgren"
    }

Then another question comes up, can we find all user who like `Mustrooms` and
`Banana`? We may try this:

    r.db("foodb").table("users")
      .getAll('Kiwi', 'Banana', {index: 'favfoods'})

However, that's an `or`. RethinkDB will return documents where its index value
matches either `Kiwi` or `Banana`.

Even more complex, we want to find user who like Kiwi most. Meaning `kiwi` has
to be first element in ther *favfoods* array.

To do that, we see that we are passing business logic into RethinkDB. We have to
somehow represent that logic in RethinkDB, calculate the value, and index the
return value. Let's meet `arbitrary expressions` index. 

## Arbitray expressions index

As its name, the returned value of expression is used to calculate index.

Unfortunately at this moment, we cannot simple answer who like both of banana
and kiwi. We will save it for later chapter when we learn about `map` function.
Now, let's find users
who like kiwi the most. With assumption that the first element of `favfoods` array
is what an user like most.

    r.db("foodb").table("users")
      .indexCreate('most-favourite-food', function (user) {
        return user("favfoods").nth(0)
      })

Given an array, `nth(n)` return n-th element. We call `nth(0)` on array
`favfoods`  that means first
element of array since they are zero-base index.

    r.db("foodb").table("users")
      .getAll('Kiwi', {index: 'most-favourite-food-1'})
    #=>
    {
    "favfoods": [
    "Kiwi" ,
    "Lemon" ,
    "Lime" ,
    "Coffee" ,
    "Sweet orange"
    ] ,
    "id":  "0b83164e-fb42-4273-8db1-ba12be6e580d" ,
    "name":  "Carl Achiban"
    } {
    "favfoods": [
    "Kiwi" ,
    "Banana" ,
    "Peanut" ,
    "Asparagus" ,
    "Common cabbage"
    ] ,
    "id":  "d10b51d7-d321-4b41-bd7d-1367ede0eb30" ,
    "name":  "Luma Ramses"
    }

This index is powerful because we can push more complex searching to database
engine. Let's say we want to find all user who like `Kiwi` most and is a female.

    r.db("foodb").table("users")
      .indexCreate('most-favourite-food-gender', function (user) {
        return [user("gender"), user("favfoods").nth(0)]
      })

Here, we are trying to create non-multi index. The index is an array of gender
and most favorite food item. Now, let's try our index

    r.db("foodb").table("users")
      .getAll(['f', 'Kiwi'], {index: 'most-favourite-food-gender'})
    #=>
    {
    "favfoods": [
    "Kiwi" ,
    "Lemon" ,
    "Lime" ,
    "Coffee" ,
    "Sweet orange"
    ] ,
    "gender":  "f" ,
    "id":  "0b83164e-fb42-4273-8db1-ba12be6e580d" ,
    "name":  "Carl Achiban"
    } {
    "favfoods": [
    "Kiwi" ,
    "Banana" ,
    "Peanut" ,
    "Asparagus" ,
    "Common cabbage"
    ] ,
    "gender":  "f" ,
    "id":  "d10b51d7-d321-4b41-bd7d-1367ede0eb30" ,
    "name":  "Luma Ramses"
    }

One thing I want to remind you is that the return
function in any RethinkDb expression is evaludated on RethinkDB, not on client
language. You cannot just write anything. You have to use RethinkDB expression
in return value so that RethinkDB can calculate it. In previous chapter, we
learn about `expr`, you can use that to turn native object into RethinkDb object
when needed.

Let's look at this example:

    r.table("users").indexCreate("full_name2", function(user) {
        return r.add(user("last_name"), "_", user("first_name"))
    }).run(conn, callback)

We are trying to create an index by appending *last_name* and *first_name*. We
cannot write

    return user("last_name") + user("first_name")

because ReThinkDB won't understand that expression. We have to call `r.add`
function.

However, even if you see something like 

    return user("last_name") + user("first_name")

That doesn't mean RethinkDB understands your native expression. It's actually
your driver overload operator (`+` operator) in this case because your host
language apparently support operator overloading.

If you notice, we don't pass `multi: true` in any of the above examples. Can we use
multi index with arbitray expression index? Yes, we can.

Let's say if we want to find any users who like `Kiwi` or have eaten Kiwi before. We will
create an multi index by concat array `favfoods` and `eatenfoods`


    r.db("foodb").table("users")
      .indexCreate(
        'eateen-or-like-multi', 
        r.add(r.row("eatenfoods"), r.row("favfoods"))
        , {multi: true})

Now, we can use that index:


    r.db("foodb").table("users")
      .getAll('Kiwi', {index:'eateen-or-like-multi'})
    #=>
    {
    "eatenfoods": [
    "Celery leaves" ,
    "Kiwi" ,
    "Rainbow trout" ,
    "Chinese bayberry" ,
    "Hyacinth bean" ,
    "Other sandwich"
    ] ,
    "favfoods": [
    "Honey" ,
    "Cake" ,
    "Butter substitute" ,
    "Cream" ,
    "Sugar"
    ] ,
    "gender":  "m" ,
    "id":  "808cedd5-f2ac-4724-98bc-061ee84755c9" ,
    "name":  "Forrest Jacobs"
    } {
    "eatenfoods": [
    "Jerusalem artichoke" ,
    "Conch" ,
    "Milk and milk products" ,
    "Dumpling" ,
    "Custard apple" ,
    "Sacred lotus" ,
    "Japanese walnut" ,
    "Crab"
    ] ,
    "favfoods": [
    "Kiwi" ,
    "Banana" ,
    "Peanut" ,
    "Asparagus" ,
    "Common cabbage"
    ] ,
    "gender":  "f" ,
    "id":  "d10b51d7-d321-4b41-bd7d-1367ede0eb30" ,
    "name":  "Luma Ramses"
    }
    
How about finding users that liked Kiwi most and have eaten kiwi? We just need to
change the index function, this time we will use anonymous function:

    r.db("foodb").table("users")
      .indexCreate(
        'eatean-or-like-most',
        function (user) {
          return r.add(user("eatenfoods"), [user("favfoods").nth(0)])
        }
        , {multi: true})

Actually, function is just a special case of expression where expression is the
result of function executing. Now, we can try to find:

    r.db("foodb").table("users")
      .getAll('Kiwi', {index: 'eatean-or-like-most'})
    #=>
    {
        "eatenfoods": [
            "Shrimp",
            "Other fish product",
            "Sweet orange",
            "Unclassified food or beverage"
        ],
        "favfoods": [
            "Kiwi",
            "Lemon",
            "Lime",
            "Coffee",
            "Sweet orange"
        ],
        "gender": "f",
        "id": "0b83164e-fb42-4273-8db1-ba12be6e580d",
        "name": "Carl Achiban"
    } {
        "eatenfoods": [
            "Celery leaves",
            "Kiwi",
            "Rainbow trout",
            "Chinese bayberry",
            "Hyacinth bean",
            "Other sandwich"
        ],
        "favfoods": [
            "Honey",
            "Cake",
            "Butter substitute",
            "Cream",
            "Sugar"
        ],
        "gender": "m",
        "id": "808cedd5-f2ac-4724-98bc-061ee84755c9",
        "name": "Forrest Jacobs"
    } {
        "eatenfoods": [
            "Jerusalem artichoke",
            "Conch",
            "Milk and milk products",
            "Dumpling",
            "Custard apple",
            "Sacred lotus",
            "Japanese walnut",
            "Crab"
        ],
        "favfoods": [
            "Kiwi",
            "Banana",
            "Peanut",
            "Asparagus",
            "Common cabbage"
        ],
        "gender": "f",
        "id": "d10b51d7-d321-4b41-bd7d-1367ede0eb30",
        "name": "Luma Ramses"
    }



## Checking index status

As I've said, indexing reduces write performance, therefore it takes time to
create after we issue creating command. Depend on the table size, how many
records we have, we have to wait for an amout of time before using it. We can
check the status of an index to see if it's ready to use

    r.table().indexStatus(indexName)

Such as:

    r.db("foodb").table("compounds_foods").indexStatus("food_id")
    #=> 
    {
    "blocks_processed": 656 ,
    "blocks_total": 11331 ,
    "function": <binary, 181 bytes, "24 72 65 71 6c 5f..."> ,
    "geo": false ,
    "index":  "food_id" ,
    "multi": false ,
    "outdated": false ,
    "ready": false
    }

The `ready` field indicates if the index is ready to use.

Sometimes we just want to say, when the index is ready, please run this:

    r.table.indexWait


# Using index

## Ordering

Sorting with order without index limit to 100k issue. Always considering using
an index. We already learn about `orderBy` in chapter but we haven't use index
at that time. It takes this form of syntax:

    table.orderBy([key1...], {index: index_name}) → selection<stream>
    selection.orderBy(key1, [key2...]) → selection<array>
    sequence.orderBy(key1, [key2...]) → array

On a table, that means result of a `table` command, you can pass over an index
for sorting. Example, we want to sort table *compounds_foods* by name:

If we don't use index we will get:

    r.db("foodb")
      .table("compounds_foods")
      .orderBy("name")   
    //=>
    RqlRuntimeError: Array over size limit `100000` in:
    r.db("foodb").table("compounds_foods").orderBy("name")
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Therefore, we have to create index:

    r.db("foodb")
      .table("compounds_foods")
      .indexCreate("foodname", r.row("name"))

When index is ready, the original query works if we tell it what index to use:

    r.db("foodb")
      .table("compounds_foods")
      .orderBy("name", {index: "foodname"})

By defaul, the ordering is ascending. To change to descending, we simply wrap 
the index name in `r.desc` or `r.asc`

    r.db("foodb")
      .table("compounds_foods")
      .orderBy("name", {index: r.desc("foodname")})

## Pagination

To pagination data, we will use a combination of `skip`, `limit` and `slice`.
We already learn them in *Chapter 3*. But now you can use in combination with
`orderBy` using an index to make it more efficient.

**skip(n)**

: Skip a number of element from the begining of sequence or array

**limit(n)**

: End the sequence after we read up to the give number of limit

### slice

Instead of manually doing pagination by using `skip` and `limit`, we can simply
tell RethinkDB that we want the data from this position to the other position.
Similarly to how we have an `slice` function to slice an array.

Let's take our `foods` table. 

    r.db("foodb")
      .table("foods")
      .orderBy(r.desc("name"))
      .slice(10, 12)

Will return two rows from position 10 and 11. So we can calculate the slice
index for pagination. Example, assume we have 10 items per page, so on page 7
we have item from 70 to 80. And we can do:

    r.db("foodb")
      .table("foods")
      .orderBy(r.desc("name"))
      .slice(70, 80)

T> ## Where else I can call Skip, Limit, Slice
T>
T> These command can be called on a selection, an array or stream. So you can 
T> basically call them on almost situation.

## Transform data

So far, we always taking the value that return from RethinkDB to work with it. In
any real application, you probably want to do some transform around it. If we do
it at application level, for complex thing, it makes sense. But for simple thing,
we might waste another extra loop. Sometime we want to do thing at RethinkDB
level, or we want to use transform data in other ReQL expression.

An example is the function `nth` or `count`. We call them on a sequence, or an
array of data, and they return a different data. They transform original data
into a different piece of data. But `nth` and `count` are simple function. They
don't have any complex logic inside them. Some transform function has complex
logic. With those logic, we have to use some kind of `if ...else...` command, or
loop. To help us with that, RethinkDB has some control structure functions such
as `branch` (similar to `if`) or `forEach`, `do`. RethinkDb shines in these
areas because database engine now seems to have an embedded language in it.

Let's cover more of those functions. Along the way, we will learn somne structure
control command. Now, move on to our next function, **map**.

### Map

Let's say we want to divide our users into 3 groups, who are under 18 years old
are **teenager**, between 18-50 are **adult**, and over 50 years old are
**older**. We have a pattern here, with each document in table, we want to
calculate a new data, depend on their existed data. In RethinkDB, we used
***map*** function.

Map apply a function on document, and return value of function is returned from
query. With our example, in a normal programming language such as Ruby, we can
write:

    users.map do |user|
      if user["age"] <= 18
        "teenager"
      else if user["age"] >18 && user["age"] <50 
        "adult"
      else
        "older"
      end
    end

In RethinkDB, the format of map is:

    sequence1.map([sequence2, ...], mappingFunction) → stream
    array1.map([array2, ...], mappingFunction) → array
    r.map(sequence1[, sequence2, ...], mappingFunction) → stream
    r.map(array1[, array2, ...], mappingFunction) → array

With this example, we have to represent `if` in RethinkDB function. We do that
with `branch`:

    r.branch(test, true_branch, false_branch) → any

Time to write our function:

    r.db("foodb").table("users").map(function (user) {
      return r.branch(
        user("age").lt(18),
        "teenager",
        r.branch(
          user("age").gt(50),
          "older",
          "adult"
        )
      )
    })
    #=>
    "older" "older" "older" "adult" "older" "older" "older" "adult" "older"
    "older" "older" "adult" "teenager" "adult" "adult" "adult" "older" "adult"
    "older" "older" "older" "older" "adult" "teenager" "adult" "older" "older"
    "older" "older" "adult" "adult" "older" "adult" "older" "older" "older" "adult"
    "older" "older" "older"

Yay, we get what we want. But it only returns the value from fuction, we don't
know who is who. Well, that's `map` job. It transforms the whole document into
the return value. How about we return an object, with original `name` field, and
our `group` field, like this:

    r.db("foodb").table("users").map(function (user) {
      return {
        name: user("name"), 
        group: r.branch(
        user("age").lt(18),
        "teenager",
        r.branch(
          user("age").gt(50),
          "older",
          "adult"
        )
        )}
    })
    #=>
    {
    "group":  "adult" ,
    "name":  "Arthur Hegmann"
    } {
    "group":  "older" ,
    "name":  "Ricky Quigley Sr."
    } {
    "group":  "older" ,
    "name":  "Jazmyne Brakus"
    }
    ....


Great. But if we want to return whole document, with extra `group` field, do we
have to repeatedly write every fields? Well, let's meet `merge`.

    r.db("foodb").table("users").map(function (user) {
      return user.merge({
        group: r.branch(
          user("age").lt(18),
          "teenager",
          r.branch(
            user("age").gt(50),
            "older",
            "adult"
          )
        )})
    })

If you wondering whether we can use `row` in `map` expression instead of using function, I would say: Yes, we can. But we cannot do that in the above example because `row` doesn't work in
nested query. In above example, we nested `r.branch` inside `merge`, and inside
other `branch`. 

Let's see an example where we can use `r.row`: couting how many foods they have
eaten:

    r.db("foodb").table("users").map({
      name: r.row("name"),
      total_eaten: r.row("eatenfoods").count()
    })
    #=>
    {
    "name":  "Arthur Hegmann" ,
    "total_eaten": 6
    } {
    "name":  "Ricky Quigley Sr." ,
    "total_eaten": 7
    }

Inside `map` function, we can use any arbitrary ReQL command to fetch data. In
follow example, we try to use `getAll` and an index to count data. Let's find
the quantity of flavor which a compound has. Table *compounds* has an associated
table *compounds_flavors* which stores a releation compounds and flavors using
two fields: *compound_id* and *flavor_id*. By counting how many item for a
given *compound_id* on table *compounds_flavors*, we can get the total flavor
count of compound.

    r.db('foodb')
      .table('compounds')
      .map(function(doc) {
        return {
          compound_id: doc('id'),
          name: doc('name'),
          flavor_total: r.db('foodb').table('compounds_flavors').getAll(doc('id'), {index: 'compound_id'}).count()
        }
      })
      .orderBy(r.desc('flavor_total'))
    //=>
    [
      {
          "compound_id": 3266,
          "flavor_total": 22,
          "name": "Ethyl methyl sulfide"
      },
      {
          "compound_id": 930,
          "flavor_total": 19,
          "name": "3-Ethylpyridine"
      },...
    ]

In this example, with each of compound, we pass it into `map` function. The map
function count how many flavor it has by querying table **compound_flavor**. We
are using `getAll` with an index to make it run fast. We finally call
`.count()` to count the total element of sequence. In the map function, we
explicitly return an object which we construct ourselves. The using of it
someitmes confused people that the function runs on client site which is wrong.
The map function is executed on RethinkDB server. If we don't like return an
JSON object directly as above, we can use `pluck` to get only **name** and
**compound_id** and merge it with extra flavor_total field like this:

    r.db("foodb")
      .table('compounds')
      .map(function(doc) {
        return doc.pluck('name', 'compound_id').merge({
          flavor_total: r.db('foodb').table('compounds_flavors').getAll(doc('id'), {index: 'compound_id'}).count()
        })
      })
      .orderBy(r.desc('flavor_total'))

Using `map`, we can pre-calculate some data, either using function or
expression. Let's meet other kind of `mapping` function.

### concatMap

***concatMap*** is very similar to map. It applies function to every element of
sequence. But it's different since it also try to flatten, or concat all
element into a single sequence.

    r.expr([1, 2, 3]).map(function(x) { return [x, x.mul(2)] })
    #=> [[1, 2], [2, 4], [3, 6]]

However, `concatMap` will concat all sub sequence/array into final return data.

    r.expr([1, 2, 3]).concatMap(function(x) { return [x, x.mul(2)] })
    #=> [1, 2, 2, 4, 3, 6]

When is it useful? It looks like very useless, right? Well, let's look at
`favfoods` field. If we want to get a list of `favfoods` on the entire system, we
can do:

    r.db("foodb").table("users").map(
      r.row("favfoods")
    )
    #=>
    [
      "Garden tomato (var.)" ,
      "Linden" ,
      "Lowbush blueberry" ,
      "American cranberry" ,
      "Vanilla"
    ],
    [
      "Swiss chard" ,
      "Chicory roots" ,
      "Grapefruit" ,
      "Jostaberry" ,
      "Spirit"
    ]

As expected, we get an array of array. That's when `concapMap` shines:

    r.db("foodb").table("users").concatMap(
      r.row("favfoods")
    )
    #=>
    Pikeperch
    Pacific ocean perch
    True seal
    Columbidae (Dove, Pigeon)
    Conch
    Kiwi
    Lemon
    Lime
    Coffee
    Sweet orange
    Kiwi
    True seal
    Salmonidae (Salmon, Trout)
    ...

If you notice, we have duplicate data. That's natural because many users may
like the same foods. To get distinct value, you can call `distinct` on the sequence:

    r.db("foodb").table("users").concatMap(
      r.row("favfoods")
    ).distinct()
    #=>
    [
    "Abalone" ,
    "Abiyuch" ,
    "Acerola" ,
    "Acorn" ,
    "Adobo" ,
    "Adzuki bean" ,
    ]

`distinct` accepts an index and use it to differentce document, without any
specified index, it uses primary index, or the `id` value, which is good enough
in our case, because we don't have foods with the same name. Sometimes, you may want
to create extra index for the field and call `distinct` using that index.

Let's dig into a more complex example. For each of foods, let's find all of its
compound.

Let's create an index first:

    r.db("foodb").table("compounds_foods").indexCreate("food_id")

With that index, we can try our query:

    r.db("foodb")
      .table("foods")
      .withFields("id", "name")
      .concatMap(function (food) {
        return 
          r.db("foodb").table("compounds_foods")
          .getAll(food("id"), {index: "food_id"})
          .withFields("id", "food_id", "compound_id", "orig_food_common_name",
    "orig_compound_name")
          .map(function(compound_food) {
            return food.merge({compound: compound_food})
          })
      })
    #=>
    {
    "compound": {
    "compound_id": 21594 ,
    "food_id": 2 ,
    "id": 15609 ,
    "orig_compound_name":  "Fatty acids, total saturated" ,
    "orig_food_common_name":  "Cabbage, savoy, raw"
    } ,
    "id": 2 ,
    "name":  "Savoy cabbage"
    } {
    "compound": {
    "compound_id": 21595 ,
    "food_id": 2 ,
    "id": 15610 ,
    "orig_compound_name":  "Fatty acids, total mono-unsaturated" ,
    "orig_food_common_name":  "Cabbage, savoy, raw"
    } ,
    "id": 2 ,
    "name":  "Savoy cabbage"
    }

Note that we use some `withFields` command to limit on the fields that we want
to include in return document.

Above example won't work with `map`. Because the return value from function
isn't a single value, but another sequence. If you attempt to use `map`, you can
see error clearly:

    r.db("foodb")
      .table("foods")
      .withFields("id", "name")
      .map(function (food) {
        return 
          r.db("foodb").table("compounds_foods")
          .getAll(food("id"), {index: "food_id"})
          .withFields("id", "food_id", "compound_id", "orig_food_common_name",
    "orig_compound_name")
          .map(function(compound_food) {
            return food.merge({compound: compound_food})
          })
      })
    #=> RqlRuntimeError: Expected type DATUM but found SEQUENCE:

`concatMap` can operator on sequence value from funciton, and try to flatten it
for us. Let's get back to this and break down how it works:

    r.db("foodb")
      .table("foods")
      .withFields("id", "name")
      .concatMap(function (food) {
        return 
          r.db("foodb").table("compounds_foods")
          .getAll(food("id"), {index: "food_id"})
          .withFields("id", "food_id", "compound_id", "orig_food_common_name",
    "orig_compound_name")
          .map(function(compound_food) {
            return food.merge({compound: compound_food})
          })
      })

For every document of foods, it's transformed into a document similar to this:

    [
      {id: id1, name: name1, compound: compound_food_document1}, 
      {id: id1, name: name1, compound: compound_food_document2},
      {id: id1, name: name1, compound: compound_food_documentn},a
    ]

That mean, with a sequence of foods, we will have this(without concatMap):

    [
      {id: id1, name: name1, compound: compound_food_document1}, 
      {id: id1, name: name1, compound: compound_food_document2},
      {id: id1, name: name1, compound: compound_food_documentn},...
    ],
    [
      {id: id2, name: name2, compound: compound_food_document1}, 
      {id: id2, name: name2, compound: compound_food_document2},
      {id: id2, name: name2, compound: compound_food_documentn},...
    ],
    ...

But concat map will fallten array and we have this:

    {id: id1, name: name1, compound: compound_food_document1}, 
    {id: id1, name: name1, compound: compound_food_document2},
    {id: id1, name: name1, compound: compound_food_documentn},...
    ,
    {id: id2, name: name2, compound: compound_food_document1}, 
    {id: id2, name: name2, compound: compound_food_document2},
    {id: id2, name: name2, compound: compound_food_documentn},...
    , ...

That's power of `concatMap`. We can use it to do a join effect. But one problem
remains, as you can see, we have many document with same `food` but different
compound of that food. We can somehow compile them into an array like this:

    {id: id1, name: name1, 
      compound: [compound_food_document1, compound_food_document2, ...]
    },
    {id: id2, name: name2, 
      compound: [compound_food_documentn, compound_food_documentn, ...]
    },...

Let's tweak our `concatMap` a bit.

    r.db("foodb")
      .table("foods")
      .withFields("id", "name")
      .concatMap(function (food) {
        return [
          food.merge({compound: 
            r.db("foodb").table("compounds_foods")
            .getAll(food("id"), {index: "food_id"})
            .withFields("id", "food_id", "compound_id", "orig_food_common_name",
    "orig_compound_name")
            .limit(10)
          })
        ]
      })

First off, you notice that we wrap food.merge in array. Why so? Because
`concatMap` expects return value from function is a sequence. `map` expects
return value from function is DATUM, likewise. We also call `limit(10)` on the
`compound_foods` sequence to limit first 10 results, sort by primary key, `id`
field of `compound_foods` table in this case.

Instead of creating a map to loop over the `compound_foods` sequence and creating a
document, we simply bring the whole `compound_foods` array to merge into
**food** document. Looks good, run this and here come the error:

    RqlRuntimeError: Expected type DATUM but found SEQUENCE:

Well, merge expect a `DATUM`. A datum is like a single primitive value such as
a number, an array. However, we are passing SEQUENCE. You can understand that we are
expecting an primitive value, but we passed a cursor. Like in Ruby, when we
expect an array, but we pass an enumarator. In RethinkDB, to make this kind of
merge work, we have to explicitly convert it to an array, using `coerceTo` with
parameter **array**.

    r.db("foodb")
      .table("foods")
      .withFields("id", "name")
      .concatMap(function (food) {
        return [
          food.merge({compound: 
            r.db("foodb").table("compounds_foods")
            .getAll(food("id"), {index: "food_id"})
            .withFields("id", "food_id", "compound_id", "orig_food_common_name",
    "orig_compound_name")
            .limit(10)
            .coerceTo('array')
          })
        ]
      })
    #=>
    {
    "compound": [ ... ] ,
    "id": 2 ,
    "name":  "Savoy cabbage"
    } {
    "compound": [ ... ] ,
    "id": 15 ,
    "name":  "Wild celery"
    }

Now, with `coerceTo` function, we know that we can convert sequence into
`array`. So can we achieve this with map, instead of `concatMap`. Yes, and it's even more
simpler:

    r.db("foodb")
      .table("foods")
      .withFields("id", "name")
      .map(function (food) {
        return
          food.merge({compound: 
            r.db("foodb").table("compounds_foods")
            .getAll(food("id"), {index: "food_id"})
            .withFields("id", "food_id", "compound_id", "orig_food_common_name",
    "orig_compound_name")
            .limit(10)
            .coerceTo('array')
          })
      })

We no longer have to wrap it in `[]` because `map` can work with OBJECT. With
this example, we see that `concatMap` and `map` sometime can be used
interchangably depend on how we want to model the data.

## Index and Map

I promised you an answer to quesiton who like both of Kiwi and banana. As you
can guess, we have to create a multi index with each of value is a pair of
favourite foods. It's very hard to do this without `map`. We are thinking of
two nested loops. With map, we can do this:


    r.db("foodb")
      .table("users")
      .indexCreate("food-test-idx-1", function (food) {
        return
          food("favfoods")
            .concatMap(function (favfood) {
              return
                food("favfoods").map(function (favfood2) {
                  return [favfood, favfood2]
                  })
            })
      }, {multi: true})

We will do a map. With each of element of `favfoods, we try to create a new
array of two elements by combining the element with each of element of `favfoods`
itself. We use `concatMap` to flatten array, and set `multi: true` because this
is a multi indexer: we are returning an array from index function.

And now, we can use that index, pass a pair of value:

    r.db("foodb")
      .table("users")
      .getAll(['Spirit', 'Grapefruit'], {index: "food-test-idx-1"})
      .withFields("name", "favfoods")
    #=>Executed in 9ms. 2 rows returned
    {
    "favfoods": [
    "Swiss chard" ,
    "Chicory roots" ,
    "Grapefruit" ,
    "Jostaberry" ,
    "Spirit"
    ] ,
    "name":  "Wilburn Price"
    } {
    "favfoods": [
    "Chicory roots" ,
    "Grapefruit" ,
    "Jostaberry" ,
    "Spirit" ,
    "Abiyuch"
    ] ,
    "name":  "Audie Muller"
    }

T> Using `withFieds` to quickly pluck some fields
T>
T> I often use `withFields` to only pluck some fields that I care
T> That way the result set is easier to read

The order isn't important here, because we create the pair of array on every
element if itself. Such as with [a,b,c] array, we will have this array via map:

  [a,a], [a,b], [a,c], [b,a], [b,b], [b,c], [c, a], [c, b], [c, a]

We can elimiate [a,a], [b,b], [c,c] because they are useless. We just need to
put them in `branch` command:

    r.db("foodb")
      .table("users")
      .indexCreate("food-test-idx-2", function (food) {
        return food("favfoods").concatMap(function (favfood) {
          return food("favfoods").map(function (favfood22)    {
            return r.branch(favfood.eq(favfood2),
                [],
                [favfood, favfood2]
                )
          })
        })
      }, {multi: true})

With each element, if we see the value of two elements are the same, we return an
empty array, otherwise, we return combination array of them.

This runs fast but it has its own limitation. We cannot create more than 256
index on multi index. Other approach, not as fast, but scale better is to use
double finding. First, we find all documents match an value, from that result, we find 
all documents match second value. First we `getAll` using an index, then `filter` it.

Let's try it again. First create an index:

    r.db("foodb")
      .table("users")
      .indexCreate("food-test-idx-3", r.row('favfoods')
      , {multi: true})

    r.db("foodb")
      .table("users")
      .indexCreate("food-test-idx-3", function (food) {
          return food["favfood"]
        })
      }, {multi: true})

Now, we firstly using that index to find all user who like first fruit, then use
`filter` to filter out user who also like second fruits:

    r.db("foodb")
      .table("users")
      .getAll("Spirit", {index: "food-test-idx-3"})
      .filter(r.row("favfoods").contains("Grapefruit"))
      .withFields("name", "favfoods")
    //=>Executed in 9ms. 2 rows returned
    [
      {
      "favfoods": [
      "Swiss chard" ,
      "Chicory roots" ,
      "Grapefruit" ,
      "Jostaberry" ,
      "Spirit"
      ] ,
      "name":  "Wilburn Price"
      } {
      "favfoods": [
      "Chicory roots" ,
      "Grapefruit" ,
      "Jostaberry" ,
      "Spirit" ,
      "Abiyuch"
      ] ,
      "name":  "Audie Muller"
      }
    ]

In the result of `getAll`, we run a `filter` on it. If the result of `getAll`
contains more than 100,000 element. This method won't work. We cannot continue
to call `getAll` with specifiying an index from another `getAll` because it
returns *"SELECTION<STREAM>"*. `getAll` works only on a table.

    r.db("foodb")
      .table("users")
      .getAll(['Spirit', 'Grapefruit'], {index: "food-test-idx-1"})
      .typeOf()
    //=>
    "SELECTION<STREAM>"

## The important map/concatMap

On the surface, `filter` is powerful because they accept a function and evaluate
that function to filter data based on its result. It's very similar to array
filter that we usually has as language standard library.

The only downside is slowness which is understandable. Therefore, we usually have
to use `getAll` to leverage index. But `getAll` only query data based on value
of index which doesn't give the flexible of `filter`. Because we have to calculate
an index function to generate index, where as filter evaluated dynamically.

Considering this example. Assume we have an *orders* table contains order. And
other *orderItems* table which contains items of an order.

We can creata some data:

    r.tableCreate("orders")
    r.tableCreate("orderItems")

    r.table("orders").insert([
      {id:1, shipped: 1},      {id:2, shipped: 0},      {id:3, shipped: 1},
    ])

    r.table("orderItems").insert([
      {id:1, name: "f1", orderId: 1},
      {id:2, name: "f2", orderId: 1},
      {id:3, name: "a1", orderId: 2},
      {id:4, name: "f2", orderId: 2},
      {id:5, name: "a3", orderId: 2},
      {id:6, name: "b3", orderId: 3},
      {id:7, name: "b4", orderId: 3},
      ])

Question: find all items of every ship order. With filter, we can quickly do this:

    r.table('orderItems')
      .filter(function(orderItem) {
        return r.table('order').get(orderItem('id'))('shipped').default(0).gt(0)
      })

However, they are bad because filter limitation. So ideally we only have
`getAll` choice. But `getAll` only returns data of the table it's using index
on. In our case, **shipped status** is stored in **orders** table, whileas we
want to fetch data of **orderItems**. In other words, `getAll` doesn't give us
data we want. We have to transform its into what we want by using `map`/`concatMap`.

Let's add two indexes:

     r.table("orderItems").indexCreate("orderId")
     r.table("orders").indexCreate("shipStatus", r.row("shipped").default(0).gt(0))

With that index, we can find all of shipper order

    r.table("orders").getAll(true, {index: "shipStatus"})

Now, we will use `concatMap` to transform the order into its equivalent
orderItem

    r.table("orders")
     .getAll(true, {index: "shipStatus"})
     .concatMap(function(order) {
          return r.table("orderItems").getAll(order("id"), {index: "orderId"}).coerceTo("array")
     })

So now, we basically have the power of `filter`. We can query whatever data
inside `concatMap` function(just to be sure to use proper index anyway). The
way of thinking is in reverse of `filter`. In `filter`, we start query on table
we want to get data. In `getAll`/`concatMap`, we start query query on the table
contains the associated condition, then using `concatMap` to somehow, join the
data across table.

`map` and `concatMap` is really important and you should take a bit time to play
around and master them.

# Wrap up

This chapter is quite long. So far we've learned about index. We know how to create:

  * Simple index
  * Compound index
  * Multi value index
  * Arbitrary expression index

We've also learned how to leverage index to sort, filter data. Two important
transform functions you also learn:

  * map: transform from a source data to other value
  * concatMap: as map, but it flatten return array.

And finally, by leveraging `map`, you can easily create multi index.
