-# 7. Aggregation

A very normal task of a database is to get some kind of calculate from a given
sequence of data. We will learn those kind of function on this chapter.

## sum, average, and count

You already knew `count`. It also has an extra useful way to count is by passing a
value or a function. When passing a value, it counts the document whose value matches
the value such as counting users who are 18 year olds:

For example, let count how many food is *Type 1* we have

    r.db('foodb').table('foods')('food_type').count('Type 1')
    //=>
    627

Here we are using nested field syntax to fetch *food_type* field and count how many value that match *Type 1*.

Or count how many user who are *18* years old.

    r.db("foodb")
      .table("users")("age")
      .count(18)

We can also pass a ReQL expression or a function. Let's
find all food whose name starts with *L*

    r.db('foodb').table('foods').count(r.row('name').match('^L'))
    //=>
    31

We can also passing a value or a function to `count`
so RethinkDB only count documents match the value or when predicate function
returns true.

    r.db('foodb').table('foods')
      .count(function(food){
        return food('name').match('^L')
        })
    //=>
    31

In RethinkDB, it's very flexible on how we do thing. Such as, counting, in
fundamental is just count the element from an array. With some smart
combination we can `count` same thing in different way:

    r.db('foodb').table('foods').map(r.row('name').split('').nth(0)).count('L')

Basically we get food name, split charater one by one using `split` command,
then call `nth(0)` to fet first document. Using `map` we transform food name
table into a stream of first charater of food name, then we count this stream
for how many element equal *L*.

In a sense, passing a function is like a shortcut of filter with that function,
and count the return sequence

Below example, counting users who are 19 years old and name starts with a K:

    r.db("foodb")
      .table("users")
      .count(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
    //=>
    1

If we run `filter`, before `count`:

    r.db("foodb")
      .table("users")
      .filter(function(user) {
        return user("age").eq(23).and(user("name").match("^L"))
      })
      .count()
    //=>
    1

We got same result but it feel very redundant.

We can also pass a ReQL expression which is evaluate to true or false:

    r.db("foodb")
      .table("users")
      .count(r.row("rowage").eq(23).and(r.row("name").match("^L")))
    //=>
    1

`sum`, and `averag` is similar to `count` on how you use them. Just different
on what they give you. `sum` give you sum of sequence, and `average` does 
*average*. Let's find out how many bytes of storage need to store food image.
Each of document in *foods* table has a *picture_file_size* field store in
byte.

    r.db('foodb').table('foods').sum('picture_file_size')
    //=>
    123463051

We can also sum direcly on the value of stream

    r.db('foodb').table('foods')('picture_file_size').sum()

The key thing is that you understand how those function operate. By default,
they operator on the whole document. That's why we have to use `sum('pictire_file_size')`
when we call `sum` direcly on table. However, when we already use bracket to
get the field, we can simply call `sum()` without any parameters.

    r.db('foodb').table('foods')('picture_file_size').sum()

So you can guess what is the average file size, let's find out it:

    r.db('foodb').table('foods')('picture_file_size').avg()
    //=>
    147155.0071513707

We can also passing a function to `sum` or `avg`. In that case, RethinkDb calls
the function on every document, then get the result and use them for sum
purpose.

Let's say we only interested in filesize which is bigger than 4MB

    r.db('foodb').table('foods').sum(function(food) {
      return r.branch(
       food('picture_file_size').gt(1024*1024*4),
       food('picture_file_size'),
        0)
    })
    //=>
    9666379

Here, we are using `branch` as a normal `if else` block:

    if food('picture_file_size') > 1024 * 1024 * 4
      return food('picture_file_size')
    else
      0
    end

In some way, by passing function to `sum`, we have a simple effect of filter.
If we uses `filter`, we can write above query again

    r.db('foodb').table('foods').filter(function(food) {
      return food('picture_file_size').gt(1024 * 1024 * 4)
    }).sum('picture_file_size')
    //=>
    966379

In this case, we first find and return only documents where its
*picture_file_size* is greater than 4MB. Then we simply `sum` field
*picture_file_size* of all documents.

Basically, by passing function into `sum`, `avg`, we can transform document to
our desire result for doing `sum` or `avg` on it.

Doing some calculation is fun, but what if we want which food has smallest or
biggest picture file size. Let's move to `min` and `max`

## min and max

As their name, given a sequence, they find the minimum document. But how we
compare a JSON document? Therefore, we have to pass a *field name* to min,
the value of that field of document is used to compare among documents. If
we pass a function, the function is called on every document, the return value
is used to compare among documents.

    r.db('foodb').table('foods').max('picture_file_size').pluck('name', 'picture_file_size')
    //=>
    {
      "name":  "Meatball" ,
      "picture_file_size": 5102677
    }

We can also pass expression, the value of expression is used for comparing.
Let's find the compound which has most *health effect*.

As you can guess, RethinkDB often runs faster if we pass a field name because
no extra processing is made. In case of function, function has to be executed.
Example, let's try to get max file size of food in group *Type 1*.

    r.db('foodb').table('foods').max('picture_file_size').pluck('name', 'picture_file_size')
    //=>
    {
    "name":  "Meatball" ,
    "picture_file_size": 5102677
    }

In case of `min`, `max` they returns full document, but using a value to
compare. That hints that `min`, `max` may accept an index as comparing value.

Take this example. Try to find the *compounds* with bigest *msds_file_size*

    r.db('foodb').table('compounds').max('msds_file_size')
    //=>
    1 row returned in 217ms.

> Note that if you try `max` again without index, in second time the query
> run faster because a part of data was cached by RethinkDB. 
> The size of this cache is defined by this forumula
> (available_mem - 1024 MB) / 2.
> with available_mem is the memory when RethinkDB starts.

This runs on a SSD. Pretty slow. Now, see how fast it's compare with index

    r.db('foodb').table('compounds').indexCreate('msds_file_size')

Using this index to query, we can see it much faster.

    r.db('foodb').table('compounds').min({index: 'msds_file_size'})
    //=>
    1 row returned in 8ms.

By passing an secondary index to `min`, `max` function, the index value is used
to compare, run much faster and more efficient.

For complex logic, we can event pass a function to `min` or `max`, the return
values are used to compare. Let's find the food that has most compounds. The
compound of food is stored in *compound_foods* table.

    //First, let create an index, you can ignore if you created index before.
    r.db('foodb').table('compounds_foods').indexCreate('food_id')

    r.db('foodb').table('foods')
      .max(function(food) {
        return r.db('foodb').table('compounds_foods')
                .getAll(food('id'), {index: 'food_id'})
                .count()
      })
    //=>
    1 row returned in 1min 8.2

Yay, it runs in 1 minute and 8.2 seconds. Super slow. Because the function has
to be run on every document. That's being said, complex function may be slow,
but they are useful when we need it.

## distinct

Given a sequence, `distinct` remove duplication from it. When giving an index,
the duplication is detected by value of index. Its syntax is:

    sequence.distinct() â†’ array
    table.distinct([:index => <indexname>]) â†’ stream

As you can tell, whenever we return an array, we will run into 100,000 element
isues if the return array has more than 100,000 elements. So keep that in mind
and try to call distinct with a proper index, which we will learn quickly

Let's start with this simple example:

    r.expr([1, 2, 3, 4, 1]).distinct()
    //=> 4 rows returned
    [
      1 ,
      2 ,
      3 ,
      4
    ]

Let's get a list of our *username*, without duplication

    r.db("foodb")
      .table("users")
      .withFields("name")
      .distinct()
    //=>Executed in 30ms. 152 rows returned
    [
    {
    "name":  "Abe Willms"
    } ,
    {
    "name":  "Adela Klein V"
    } ,...]


Let's see what kind of `age` our users are:

    r.db("foodb")
      .table("users")
      .withFields("age")
      .distinct()
   //=> 71 rows are returned
    [
    {
    "age": 9
    } ,
    {
    "age": 10
    } ,
    {
    "age": 13
    } ,
    {
    "age": 14
    } ,]

So we have 152 unique user's name and 71 unique `age`.

Sometime, the way to detect duplication is not by comparing value of a single
field but the result of some logic. We can build an index based on that logic.
Then using value of index to run distinct on.

Imagine we want to divide user into 3 groups, depend on their *age*

    r.db("foodb")
      .table("users")
      .indexCreate("age-group", function (user) {
        return
          r.branch(
            user("age").lt(18),
            "teenager",
            r.branch(user("age").gt(50),
              "older",
              "adult"
            )
          )
      })

With that index, we can quickly list age group of our users, by
calling `distinct` on table, passing that index:

    r.db("foodb")
      .table("users")
      .distinct({index: "age-group"})
    //=> 3 rows return
    "adult"
    "older"
    "teenager"

When passing index, the value of index is returned; and therefore that value is
used to detect duplication.

Let's try on some big table: list all *orig_food_common_name* of *compound_foods*.

    r.db('foodb').table('compounds_foods')('orig_food_common_name').distinct()
    //=>
    9492 rows returned in 41.44s.

Here we use bracket to return only *orig_food_common_name* field, then remove
duplication with `distinct`. The query runs in **41.44** seconds. It also
returns the whole array, **with 9492 rows**, means all data has to be put into
memory and transfer over network. To make it faster, more efficient, we can
use index, and the result will be a stream.

Let's create an index for that field.

    r.db('foodb').table('compounds_foods').indexCreate('orig_food_common_name')

We passing an argument *index: index_name* to distinct

    r.db('foodb').table('compounds_foods').distinct({index: 'orig_food_common_name'})
    //=>40 rows returned in 161ms. Displaying rows 1-40, more available

We optimize it from *41.44s* to *161ms*!!! So fast. It has two reason for this
to be fast.

  1. We using an index as value for distinct to find the difference
  2. A stream is return. So the whole array won't have to load into memory and transfer to client

Basically, without an index, RethinkDB has to scan the whole table. It is slow in two ways:

  * slow to fetch data: read whole table, no index is used
  * slow to return data: a big amout of data transfer overnetwork from RethinkDB to client

When we pass an index, RethinkDB pickup the value of index, it doesn't have to
care or load the whole document. The return result is stream, so client receive
a cursor to fetch data lazily.

As you see, the command we learn on this chapter is operating on the whole
sequence. However, usually coming with aggregation is grouping. We want to
divide a sequence into many group, and doing aggregation on those group.
To do that, let's learn about group

## group

Yeah, group is everywhere. Group command groups data into many sub sequence,
we can continue to run aggeration on those sub sequence. For example, instead
of counting the whole sequence. We want to count how many document are in group
A, how many document in group B and so on, with group A or group B is document
that share same particular value.

Let's see how it is handle in RethinkDB.

    sequence.group(fieldOrFunction..., [{index: "indexName", multi: false}]) â†’ grouped_stream

In a nut shell, taking a sequence, depend on the value of field or return value
of function, RethinkDB groups documents with same value into a group.

Looking at `flavors` table, let's group them by their `flavor_group` field:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
    #=>
    [{
        "group": "animal",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "animal",
                "id": 112,
                "name": "animal",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561018,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "balsamic",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 43,
                "name": "others",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561011,
                    "timezone": "-07:00"
                },
                "updater_id": null
            },
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "balsamic",
                "id": 40,
                "name": "chocolate",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561010,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    {
        "group": "camphoraceous",
        "reduction": [
            {
                "category": "odor",
                "created_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "creator_id": null,
                "flavor_group": "camphoraceous",
                "id": 101,
                "name": "camphoraceous",
                "updated_at": {
                    "$reql_type$": "TIME",
                    "epoch_time": 1317561017,
                    "timezone": "-07:00"
                },
                "updater_id": null
            }
        ]
    },
    ...]

The returned array includes two fields:

  * group: value of group field, in our case is value of `flavor_group` field
  * reduction: an array contains all of document has same value for `flavor_group` field

When we continue to chain function after `group`, the function will operate on `reduction` array.
The result of function will replaced the value of reduction array.

For example, we can count how many element of **reduction**:

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "animal" ,
    "reduction": 1
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    } ,
    {
    "group":  "camphoraceous" ,
    "reduction": 1
    } ,
    ...
    ]

So, the *reduction* field no longer contains an array of document, but contains
value of how many document in original reduction array.

T> ## Command chain after group runs on grouped array
T>
T> It's important to understand `group` make next function call operator
T> on its `reduction` field.

Similarly, instead of counting, say we care about the first document only.

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .nth(0)
    //=>
    [
    {
    "group": null ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group": null ,
    "id": 148 ,
    "name":  "cotton candy" ,
    "updated_at": Sun Oct 02 2011 06:12:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "animal" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "animal" ,
    "id": 112 ,
    "name":  "animal" ,
    "updated_at": Sun Oct 02 2011 06:10:18 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    {
    "group":  "balsamic" ,
    "reduction": {
    "category":  "odor" ,
    "created_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "creator_id": null ,
    "flavor_group":  "balsamic" ,
    "id": 40 ,
    "name":  "chocolate" ,
    "updated_at": Sun Oct 02 2011 06:10:10 GMT-07:00 ,
    "updater_id": null
    }
    } ,
    ...
    ]

Here, `nth(0)` will be call on *reduction* array, return its first element, and
re-assign the result to *reduction* field.

W> Why null group?
W>
W> Why do we have a value ***null*** here. It's because
W> some documents don't have any value for `flavor_group`,
W> or in other words, a NULL value. They are put into the same group
W> of NULL

Note that we have some limitations with `group` where the `group` size is 
over 100000 elements. For example, let's group `compounds_foods` by their
`orig_food_common_name`

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')

And we got this:

    RqlRuntimeError: Grouped data over size limit `100000`.  Try putting a reduction (like `.reduce` or `.count`) on the end in:
      r.db("foodb").table("compounds_foods").group("orig_food_common_name")

Why so? Because when we end the chain with `group`, the whole array is loaded
into memory, and our sequence are greater than 100000 elements. We have around
~668K documents. However, when we call `reduce` or `count` on it, the number
of documen will be reduce. RethinkDB won't have to keep them all into memory and
makes it works.

Let's try what it suggests:

    r.db("foodb")
      .table("compounds_foods")
      .group('orig_food_common_name')
      .count()
    #=>
    //Executed in 45.03s. 9492 rows returned
    [
    {
        "group": null,
        "reduction": 4313
    },
    {
        "group": "AMARANTH FLAKES",
        "reduction": 68
    },
    ...
    ]

Now, this result is of course not what we expected. But why it runs? It is
because when we call `count`, the whole array of `reduction` field becomes
a single value instead of an array of grouped data, that makes the size of
final group data smaller. As you can see, ***9492*** is returned, and
that's all loaded into memory.

So keep in mind that we have some limitation with `group`. If you notice,
*9492* is the same number when we run `distinct` on *orig_food_common_name*
field.

    r.db('foodb').table('compounds_foods')('orig_food_common_name').distinct()
    //=>
    9492 rows returned in 41.02s.

They return same amount of document because while they are different, they
share same concept of equality. Look at its define a gain:

  * group: group many documents which has same value of field or function result into a single document.
  * distinct: eliminate duplication, based on the value of a field or function result.

While they return different data, they retrun same quantity of document.
*Distinct* elimiate equality by remove and keeping one. *Group* eliminate
equality by mergering them into one.

Above result confirms that our query works properly. Sometimes, it's fun to go back
and try different query as a way to validate our queries.

## ungroup

As you can see, anything follow `group` operates on sub stream, or `reduction` array.
Can we make the follow function run on returned sequence of `group` itself? Such as,
we want to sort by the value of `reduction` field. Let's try to sort `flavors` by
how many value of `flavor_group`

    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
      .ungroup()
      .orderBy(r.desc('reduction'))
    //=>
    [
    {
    "group": null ,
    "reduction": 743
    } ,
    {
    "group":  "fruity" ,
    "reduction": 24
    } ,
    {
    "group":  "floral" ,
    "reduction": 14
    } ,
    {
    "group":  "balsamic" ,
    "reduction": 10
    },...
    ]

Without `ungroup`, We will get an error


    r.db("foodb")
      .table("flavors")
      .group('flavor_group')
      .count()
      .orderBy(r.desc('reduction'))
    //=>
    e: Cannot convert NUMBER to SEQUENCE in:
    r.db("foodb").table("flavors").group("flavor_group").count().orderBy(r.desc("reduction"))

Error occurs because without `ungroup`, `orderBy` is called on reduction array,
in this case it's a single number(the quantity of document in reduction array)
and `orderBy` cannot work on a single number.

So `ungroup` turns the return array from `group` into a sequence of object, with each object
includes 2 fields:

    * group: the value is used for grouping
    * reduction: all document contains same `group` value

and let any subsequent commands follows it operate on the whole sequence instead
of sub sequence from `group`. That's why it's call `ungroup` because it won't
treat value of `reduction` field as a sub sequence to work on, the whole `reduction`
array are now just a normal array of a document of the sequence.

Let's dive into this example to learn more about it. Let's say we have an array
of value, we want to get sub of *odd* number and *even* number.
Using `expr` we can easily represent an array in RethinkDB

    r.expr([1, 2, 14, 4, 3, 1, 7, 12, 10, 9, 3, 5])

To denote *odd* number and *even* we can group them by the result of mod to 2

    r.expr([4, 3,5, 2])
    .group(r.row.mod(2))
    //=>
    [{
        "group": 0,
        "reduction": [
            4,
            2
        ]
    }, {
        "group": 1,
        "reduction": [
            3,
            5
        ]
    }]

Now, a `sum` command follow will work on value of `reduction`.

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .sum()
    //=>
    [
      {
      "group": 0 ,
      "reduction": 6
      } ,[
      {
      "group": 1 ,
      "reduction": 8
      }
    ]

Nothing is new here. We know what `group` and `sum` do. 
The *group 0* has reduction = [4,2] so the sum is 6.

If we put a ungroup first

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .ungroup()
    //=>
    [{
        "group": 0,
        "reduction": [
            4,
            2
        ]
    }, {
        "group": 1,
        "reduction": [
            3,
            5
        ]
    }]

The output is same but the context is changed. Using `typeOf` we can verify:

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .ungroup().typeOf()
    //=>
    ARRAY

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .typeOf()
    //=>
    GROUPED_STREAM

So after `ungroup`, the result became an array instead of grouped_stream. Now,
if we call `sum`, sum will work on the whole documents, that mean document with
two fields *group* and *reduction* instead of sub stream of *reduction*

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .ungroup()
      .sum()
    //=>
    e: Expected type NUMBER but found OBJECT in:
    r([4, 3, 5, 2]).group(r.row.mod(2)).ungroup().sum()

We can confirm that the `ungroup` really operator on the whole document:

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .ungroup()
      .sum(r.row('reduction').sum())
    //=>
    14

So with each document, we get the `sum` of field `reduction`, which is an
array, then sum them all. The final result is: (4+2) + ( 3+5) = 14

Or, we want to show the sum of *odd* number first, right now, the sum of 
even number show up first:

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
    //=>
    [{
        "group": 0,
        "reduction": [
            4,
            2
        ]
    }, {
        "group": 1,
        "reduction": [
            3,
            5
        ]
    }]

Using `orderBy` as below wont' work:

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .orderBy(r.desc('group'))

Because reduction array has no field `group` for *orderBy* to work on.
If we use `ungroup` here, we will have access to the whole document normally:

    r.expr([4, 3,5, 2])
      .group(r.row.mod(2))
      .ungroup()
      .orderBy(r.desc('group'))

The idea of `ungroup` confused me a bit at beginning. If you are able to get it
right quickly then congrat. Otherwise, just re-read and do sime simple example
yourself and you will get it.

Now, let's move on to a even more confusing function `reduce`

## reduce

I think at this point, you already know what `count()` does. From a sequence, or an array,
it returns a single number of how many items the sequence holds. It transforms a whole
array into a single value, unlike `map` which turns each element of sequence into
other value, and returns a new sequence with all return value from `map` function. 

`count` is an example of `reduce`. `reduce` accepts a function, let's call it `reduce_function`,
and produces a single value, by repeating a function with input is the previous output of 
`reduce_function`. The `reduce_function` can be called with those parameters:

  * two elements of sequence
  * one element of sequence and one result of previous reduce_function execution
  * two results of previous reductions.

We can say that, on the first execution, two first elements of 
sequence are passed into reduce function; on the second execution, one parameter is
third element of sequence, other parameter is the result of reduce function call on
first and second element. And so on for 4th, 5h execution...

But why do we have two results of previous reductions? It's because reduced function 
can run parallel across shards and CPU core, or even across computers in a cluster.
The final result of each reduce functions on each shards or each computer,
are then passed to reduce function again, to create final result.

What will happen if the sequence has a single element? We don't have enough input
for reduce function. That is a special case, and RethinkDB will simply return 
value of element as result of reduce function.

Usually, `reduce` will be used with `map` to transform document into a value
that can
be aggregated. As you have seen, the reduce function paramters can be elements of sequence,
or the result of previous reduce function. Therefore, we need some transform  so that the
type of parameters and result of reduce function are the same, or if we don't do the 
transformation, the `reduce` function have to be able to deal with multiple data type

Take this example:

    r.expr([1, 2, 7, 8])
      .reduce(function(left, right) {
        return left.add(right)
      })
    //=>
    18

That's the sum of the array. The process of reducing is similar to these steps:

  1. reduce function is called with first element of sequence.`left=1`, `right=2`, and return `1.add(2)=3`. Result = 3
  2. reduce function is called again, with third element *7*, and result of previos call *3*
     left=3, right = 7 -> result = 10
  3. reduce function is called again, with last element of array
     left=10, right = 8 -> result = 18
  4. no more lement, return the single value of last function call => 18

W> Order of reduce function
W>
W> Above steps just to help you illustrate how it works. The real order maybe different
W> Because the data can be compute across the shard, cpu or even computer
W> and the final result of each continued to be called on reduce function
W> Never make assumption that the order of reduce function is from left to right

So `reduce` is kind of like recursion. Like in above example, writing in plain
english it looks like this:

    sum(a) = a if a contains a single element
    sum(a) = a[0] + a[1] if a has two element
    sum(a) = sum([a[0], a[1]) + sum(a[2..last]) if a has more than element

Let try another example. Find the minimun value of an array.

    r.expr([10, 12, 4, 9])
      .reduce(function(left, right) {
        return r.branch(left.lt(right), left, right)
      })
    //=>
    4

In this example, the *reduce* function returns the smaller value from two input
value. Here we use `branch` as an `if`, and `lt` to compare less than.
Here is how it runs:

  1. call function with left=10, right=12, return 10
  2. call function with left=10(previous result), right=4 , return 4
  3. call function with left=4(previous result), right=9(last element), return 4
  4. final value 4 is returned

Here, we noticed that the reduce function return same data type as input value.
Let's try to count how many document we have, using `reduce` style. Basically,
you can already guess that we will write a *reduce* function that increase to
1 while we iterates the array. But here we are writing reduce as a recursion
fuction, so we will add the *left* and *right* value.

    r.db("foodb")
      .table("flavors")
      .reduce(function(left, right) {
        return
          r.branch(left.typeOf().ne('NUMBER'), 1, left)
          .add(
          r.branch(right.typeOf().ne('NUMBER'), 1, right)
          )
      })
    //=>
    855

That the total documents of *flavors* table. Let looks at our *reduce* function again:

    function(left, right) {
        return
          r.branch(left.typeOf().ne('NUMBER'), 1, left)
          .add(
          r.branch(right.typeOf().ne('NUMBER'), 1, right)
          )
    }

*left* and *right* can be a document of
*flavors* table with its whole fields, or a number from the result of `add`
command. We use `typeOf` to detect type, if it's not a *NUMBER*, that means it
is a document, we consider that is a 1 item, and return 1 for counting. If
it's already a number, we used it, then add both number. It's just like seeing
an item, take 1, add with previous function call. Repeat this process for whole
sequence, we have a count of it.

So you see that we have to deal with `branch` command to turn the document into
a number, both for left and right. That job is a transformation, and sounds like
a job of `map`. Rewrite it we can make it cleanrer:

    r.db("foodb")
      .table("flavors")
      .map(function(doc) {
        return 1
      })
      .reduce(function(left, right) {
       return left.add(right)
      })
    //=>
    855

Now, we `map` each of document to become a single number `1`. Then the reduce
function works as a `sum` of the array. Take first two elements, return the sum.
Take the previous sum, add it with third element and so on.

Usually, we will have `map` step before `reduce` to turn document into a type
that compatible with result of *reduce* function. That's why the process of
this sometimes is called *map-reduce*.

The process of `reduce` function executing is like recursion, but with passing
the result of previous run to the function itself, we don't have to keep a
stack to store value of previous function call. In other words, that
function encapsulated its data, it doesn't access any outside variables. All
data it needs are passed to it as *left* and *right* parameter. Note that
they are just name binding, we can name them whatever, and they have to have
capability of dealing with different data type: the type of sequence element,
and the type of result of function call.

### Map Reduce

The process of *map-reduce* shines when using with `group`. When calling
`group`, the subsequent command operate on sub stream, we can take advantage
of that to run reduce on that sub stream and do the logic for our own
aggeration, by writing reduce function.

Let's try to count how many *compounds* a food has for first 10 food, using
map-reduce style instead of built-in `count` command.

Given that a food has many compounds, a compounds has many healh effects. As
in below diagram:

```
+------------+    +----------------+       +------------------+
|   food     |    |  compounds_food|       | compounds_flavors|
+------------+    +----------------+       +------------------+
|    id      +--->+   food_id      |       |                  |
|            |    |                |       |                  |
+------------+    +----------------+       |                  |
                  |  compound_id   +------>+  compound_id     |
                  +---+------------+       +------------------+
                      |
             +--------+
             |
             |    +--------------------------+
             |    | compounds_health_effects |
             |    +--------------------------+
             +--->+  compound_id             +
                  +--------------------------+
```

    r.db("foodb")
      .table("foods")
      .limit(5)
      .concatMap(function(food) {
        return
          r.db("foodb").table("compounds_foods")
            .getAll(food('id'), {index: 'food_id'})
            .pluck('food_id', 'compound_id')
          .merge({name: food('name')})
      })
      .group(function(doc) {
        return doc.pluck('food_id', 'name')
      })
      .reduce(function(left, right) {
        return
          r.branch(left.typeOf().eq("NUMBER"), left, 1)
          .add(
            r.branch(right.typeOf().eq("NUMBER"), right, 1)
          )
      })
    //=>5 rows returned in 254ms
    [{
        "group": {
            "food_id": 4,
            "name": "Kiwi"
        },
        "reduction": 378
    }, {
        "group": {
            "food_id": 20,
            "name": "Mugwort"
        },
        "reduction": 103
    }, {
        "group": {
            "food_id": 25,
            "name": "Common beet"
        },
        "reduction": 942
    }, {
        "group": {
            "food_id": 26,
            "name": "Borage"
        },
        "reduction": 225
    }, {
        "group": {
            "food_id": 30,
            "name": "Common cabbage"
        },
        "reduction": 1826
    }]

Let's improve it to remove branch command which is odd


    r.db("foodb")
      .table("foods")
      .limit(5)
      .concatMap(function(food) {
        return
          r.db("foodb").table("compounds_foods")
            .getAll(food('id'), {index: 'food_id'})
            .pluck('food_id', 'compound_id')
          .merge({name: food('name')})
      })
      .group(function(doc) {
        return doc.pluck('food_id', 'name')
      })
      .map(function(doc) {
        return 1
      })
      .reduce(function(left, right) {
        return
          left.add(right)
      })


here, we group by *food_id*, and *food name*, then for each of document of sub
stream, we map them to 1. Because we are counting, we only care about a
document as awhole instead of as individual fields. The reduce function simply
doing an sum of two left and right and return the sum. The `map` steps help us
clean up the `reduce` function because it's easier to deal with number as input,
and return number too.

Let's take a more complex example. At the same time, calculate how many flavor
and health effect a food has.

First, let create necessary index

    r.db("foodb").table('compounds_health_effects').indexCreate('compound_id')
    r.db("foodb").table('compounds_flavors').indexCreate('compound_id')
    r.db("foodb").table('compounds_foods').indexCreate('compound_id')

With the index, we can easily get all *compounds* and count how many flavor and
health effect associated with a compound.

    r.db('foodb')
      .table('compounds')
      .concatMap(function(doc) {
        return
          r.db('foodb').table('compounds_foods')
            .getAll(doc('id'), {index: 'compound_id'})
            .pluck('food_id')
            .merge({
              compound_id: doc('id'),
              flavor_total: r.db('foodb').table('compounds_flavors').getAll(doc('id'), {index: 'compound_id'}).count(),
              health_effect_total: r.db('foodb').table('compounds_health_effects').getAll(doc('id'), {index: 'compound_id'}).count()
            })
      })
    //=>400 rows returned in 1min 15.09s. Displaying rows 1-400, more available
    {
      "compound_id": 4 ,
      "flavor_total": 0 ,
      "food_id": 191 ,
      "health_effect_total": 0
    }
    {
      "compound_id": 4 ,
      "flavor_total": 0 ,
      "food_id": 189 ,
      "health_effect_total": 0
    }

In above query, we first fetch the table *compounds*, with a given compound, we
try to fetch its *food_id* by query on table *compound_foods*. A *compound*
can be in many foods, hence we used *concatMap* to flatten the return array.
We pluck field *food_id* from *compound_table* because we only care about it
instead of return the whole array.

Ok, above query give us compound and its flavor count and health effect count.
But we want to count the flavor and healh effect of a food. 
Well, a food contains many compound, so the total health effect is the sum 
of all compound's health effect.

Therefore, we can group by *food_id* field and run `reduce` function on
reduction group to get the total count

    r.db('foodb')
      .table('compounds')
      .concatMap(function(doc) {
        return
          r.db('foodb').table('compounds_foods')
            .getAll(doc('id'), {index: 'compound_id'})
            .pluck('food_id')
            .merge({
              compound_id: doc('id'),
              flavor_total: r.db('foodb').table('compounds_flavors').getAll(doc('id'), {index: 'compound_id'}).count(),
              health_effect_total: r.db('foodb').table('compounds_health_effects').getAll(doc('id'), {index: 'compound_id'}).count()
            })
      })
      .group('food_id')
      .reduce(function(left, right) {
            return {
              flavor_total: left('flavor_total').add(right('flavor_total')),
              health_effect_total: left('health_effect_total').add(right('health_effect_total')),
            }
      })
    //=> 832 rows returned in 3min 33.23s.
    {
        "group": 2,
        "reduction": {
            "flavor_total": 16,
            "health_effect_total": 517
        }
    },
    {
        "group": 3,
        "reduction": {
            "flavor_total": 0,
            "health_effect_total": 112
        }
    }

This give us a list of food id and its total flavor and health effect. Let's
make one more extra thing by returns food name too, and remap the field to make
document readable. To `map` document, so we can change field name from *group*
and *reduction*, we have to call `ungroup` first. And the final query is:

    r.db('foodb')        
      .table('compounds')
      .concatMap(function(doc) {
        return 
          r.db('foodb').table('compounds_foods')
            .getAll(doc('id'), {index: 'compound_id'})
            .pluck('food_id')
            .merge({
              compound_id: doc('id'),
              flavor_total: r.db('foodb').table('compounds_flavors').getAll(doc('id'), {index: 'compound_id'}).count(),
              health_effect_total: r.db('foodb').table('compounds_health_effects').getAll(doc('id'), {index: 'compound_id'}).count()
            })
      })
      .group('food_id')
      .reduce(function(left, right) {
            return {
              flavor_total: left('flavor_total').add(right('flavor_total')),
              health_effect_total: left('health_effect_total').add(right('health_effect_total')),
            }
      })
      .ungroup()
      .map(function(doc) {
        return doc('reduction').merge({
          food: r.db('foodb').table('foods').get(doc('group')).default({}).pluck('id', 'name'),
        })
      })

You can sit around watching the nice graph of RethinkDB admin dashboard. It
takes a few minutes depend on your CPU and disk speed. And the result:

    //=>832 rows returned in 3min 49.60s.
    [
    {
        "flavor_total": 16,
        "food": {
            "id": 2,
            "name": "Savoy cabbage"
        },
        "health_effect_total": 517
    },
    {
        "flavor_total": 0,
        "food": {
            "id": 3,
            "name": "Silver linden"
        },
        "health_effect_total": 112
    },...]

A very important point here is we start with *compounds* table instead of from
*foods* table. But later on, we group them by *food_id*. It is reverse of
fetching *foods* first, then find all *compounds* and go down all the way. It
makes the query shorter, easier to follow because we eliminate one nested level
when come directly to *compounds* table. It's important to pick the right table
to start with, and the right order.

The reason is, if we start with *food* table, we have to join/map with table
*compounds_foods* to find its compound, then join with *compounds* table, then
continue to join with two more
table *compounds_flavors* and *compounds_health_effects*. That's three levels.

When we start right at *compounds*, we just need to join with *compounds_foods 
and *compounds_flavors* and *compounds_health_effects* at the same time, because
we had *compound_id*. That is only a single depth level. We are actually done right
there, because we have enough information(*food_id* field to grouping). Then in
final step, we do a join with table *foods* food to fetch food name, but it's 
readable because the *map* is like go up a level, make query easier to follow.

Sometime you may not need to use `reduce` with `map`, `concatMap` and
some built-in command, you can already do a lot. But when you need reduce, it
really helps.

# Wrap up

When finishing this chapter, you should know how to aggregation, how to group data, counting,
and call function on grouped data. Some keys thing:

  * try to use index if possible on `min`, `max`, `distinct`
  * without `ungroup`, any chain command works on sub stream group
  * know how to use map reduce, and remember that the order of reduce function is not left to right

